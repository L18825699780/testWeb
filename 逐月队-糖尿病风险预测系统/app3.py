import streamlit as st
import joblib
import pandas as pd
import numpy as np
# noinspection PyUnresolvedReferences
import matplotlib.pyplot as plt
import seaborn as sns
import requests
import os
import json
import logging
from typing import Optional, Tuple, List, Dict, Any

# -------------------------- åˆå§‹åŒ–é…ç½® --------------------------
plt.style.use('ggplot')
sns.set_palette("husl")
st.set_page_config(page_title="ç³–å°¿ç—…é¢„æµ‹ç³»ç»Ÿ", layout="wide")

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# -------------------------- è‡ªå®šä¹‰æ ·å¼ --------------------------
st.markdown("""
<style>
/* æ¶ˆæ¯æ°”æ³¡ */
.message {
    max-width: 80%;
    margin: 10px 0;
    padding: 12px 16px;
    line-height: 1.6;
    font-size: 15px;
    border-radius: 12px;
    word-wrap: break-word;
    animation: fadeIn 0.3s ease-in-out;
}

.user-message {
    background: #f0f7ff;
    color: #0068c9;
    margin-left: auto;
    border: 1px solid #d0e3ff;
}

.ai-message {
    background: #f8f9fa;
    color: #333;
    margin-right: auto;
    border: 1px solid #eee;
}

/* æ‰“å­—æœºæ•ˆæœ */
@keyframes cursor-blink {
    0% { opacity: 1; }
    50% { opacity: 0; }
    100% { opacity: 1; }
}

.typing-cursor::after {
    content: "â–Œ";
    animation: cursor-blink 1s infinite;
    color: #666;
}

@keyframes fadeIn {
    from { opacity: 0; transform: translateY(10px); }
    to { opacity: 1; transform: translateY(0); }
}

/* ç¦ç”¨çŠ¶æ€ */
.stButton button:disabled {
    opacity: 0.5;
    cursor: not-allowed;
}

/* æ–°å¢ï¼šæ¨èé—®é¢˜æ ·å¼ */
.related-questions {
    background: #f8f9fa;
    border-radius: 8px;
    padding: 12px 15px;
    margin: 10px 0;
    border: 1px solid #eee;
}

.related-questions h4 {
    color: #2F4F4F;
    margin-bottom: 8px;
    font-size: 14px;
}

.related-questions ul {
    padding-left: 20px;
    margin: 5px 0;
}

.related-questions li {
    margin: 5px 0;
    cursor: pointer;
    transition: all 0.2s;
}

.related-questions li:hover {
    color: #0068c9;
}

/* å¥åº·å¡ç‰‡ */
.health-card {
    background: #f8f9fa;
    border-radius: 8px;
    padding: 15px;
    margin: 10px 0;
    border: 1px solid #eee;
}
</style>

<style>
/* æ–°å¢ï¼šæ¨èé—®é¢˜æŒ‰é’®æ ·å¼ */
.stButton button {
    background: none !important;
    border: none !important;
    box-shadow: none !important;
    text-align: left !important;
    padding: 8px 12px !important;
    margin: 2px 0 !important;
    color: #444 !important;
    font-weight: normal !important;
    border-radius: 8px !important;
    transition: all 0.2s !important;
    justify-content: flex-start !important;
}

.stButton button:hover {
    background: #f0f7ff !important;
    color: #0068c9 !important;
    transform: translateX(5px);
}

.stButton button:active {
    transform: scale(0.98);
}

/* é—®é¢˜å®¹å™¨æ ·å¼ */
div[data-testid="stHorizontalBlock"] > div:first-child {
    padding-left: 15px !important;
}
</style>
""", unsafe_allow_html=True)


# -------------------------- æ ¸å¿ƒåŠŸèƒ½ --------------------------
@st.cache_data
def load_dataset() -> pd.DataFrame:
    """åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®é›†"""
    try:
        df = pd.read_csv(r"ç³–å°¿ç—…é£é™©é¢„æµ‹/é€æœˆé˜Ÿ-ç³–å°¿ç—…é£é™©é¢„æµ‹ç³»ç»Ÿ/project/project_1/data-01.csv")
        df["é£é™©ç­‰çº§"] = df["Outcome"].map({0: "ä½é£é™©", 1: "é«˜é£é™©"})
        return df
    except Exception as e:
        logging.error(f"åŠ è½½æ•°æ®é›†å¤±è´¥: {str(e)}")
        st.error("æ— æ³•åŠ è½½æ•°æ®é›†ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„")
        return pd.DataFrame()


@st.cache_resource
def load_models() -> Tuple[Optional[Any], Optional[Any]]:
    """åŠ è½½æœºå™¨å­¦ä¹ æ¨¡å‹å’Œé¢„å¤„ç†å·¥å…·"""
    try:
        model = joblib.load(r"ç³–å°¿ç—…é£é™©é¢„æµ‹/é€æœˆé˜Ÿ-ç³–å°¿ç—…é£é™©é¢„æµ‹ç³»ç»Ÿ/project/project_1/random_forest_model.pkl")
        scaler = joblib.load(r"ç³–å°¿ç—…é£é™©é¢„æµ‹/é€æœˆé˜Ÿ-ç³–å°¿ç—…é£é™©é¢„æµ‹ç³»ç»Ÿ/project/project_1/scaler.pkl")
        return model, scaler
    except Exception as e:
        logging.error(f"åŠ è½½æ¨¡å‹å¤±è´¥: {str(e)}")
        st.error("æ— æ³•åŠ è½½æ¨¡å‹æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„")
        return None, None


def initialize_session_state():
    """åˆå§‹åŒ–ä¼šè¯çŠ¶æ€"""
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = []
    if 'prediction_data' not in st.session_state:
        st.session_state.prediction_data = None
    if 'is_responding' not in st.session_state:
        st.session_state.is_responding = False


# -------------------------- æµå¼èŠå¤©åŠŸèƒ½ --------------------------
def generate_related_questions(message: str) -> List[str]:
    """æ ¹æ®æ¶ˆæ¯å†…å®¹ç”Ÿæˆæ¨èé—®é¢˜"""
    question_map = {
        "ç—‡çŠ¶": [
            "è¿™äº›ç—‡çŠ¶ä¼šè‡ªè¡Œæ¶ˆå¤±å—ï¼Ÿ",
            "å¦‚ä½•åŒºåˆ†æ™®é€šç—‡çŠ¶ä¸ç³–å°¿ç—…ç—‡çŠ¶ï¼Ÿ",
            "å‡ºç°ç—‡çŠ¶åå¤šä¹…éœ€è¦å¤æŸ¥ï¼Ÿ"
        ],
        "é¥®é£Ÿ": [
            "ç³–å°¿ç—…æ‚£è€…èƒ½åƒæ°´æœå—ï¼Ÿ",
            "é€‚åˆç³–å°¿ç—…æ‚£è€…çš„é£Ÿè°±æœ‰å“ªäº›ï¼Ÿ",
            "éœ€è¦å®Œå…¨æˆ’ç³–å—ï¼Ÿ"
        ],
        "è¿åŠ¨": [
            "ä»€ä¹ˆè¿åŠ¨æœ€é€‚åˆç³–å°¿ç—…æ‚£è€…ï¼Ÿ",
            "è¿åŠ¨å‰åéœ€è¦æ³¨æ„ä»€ä¹ˆï¼Ÿ",
            "æ¯å¤©éœ€è¦è¿åŠ¨å¤šé•¿æ—¶é—´ï¼Ÿ"
        ],
        "æ£€æŸ¥": [
            "éœ€è¦åšå“ªäº›æ£€æŸ¥æ¥ç¡®è®¤ï¼Ÿ",
            "æ£€æŸ¥å‰éœ€è¦ç©ºè…¹å—ï¼Ÿ",
            "å¤šä¹…éœ€è¦å¤æŸ¥ä¸€æ¬¡ï¼Ÿ"
        ]
    }

    default_questions = [
        "æ—©æœŸå‘ç°åå¦‚ä½•è‡ªæˆ‘ç®¡ç†ï¼Ÿ",
        "è¿™äº›ç—‡çŠ¶ä¼šéšç€æ—¶é—´åŠ é‡å—ï¼Ÿ",
        "å®¶äººéœ€è¦ä¸€èµ·åšæ£€æŸ¥å—ï¼Ÿ"
    ]

    questions = []
    for keyword in question_map:
        if keyword in message:
            questions.extend(question_map[keyword][:2])  # æ¯ä¸ªç±»åˆ«æœ€å¤šå–2ä¸ªé—®é¢˜

    # ä¿è¯è‡³å°‘æœ‰3ä¸ªé—®é¢˜
    questions = (questions + default_questions)[:3]
    return questions


def render_chat_messages():
    """æ¸²æŸ“èŠå¤©æ¶ˆæ¯å†å²ï¼ˆå·²æ·»åŠ ç‚¹å‡»é—®é¢˜åŠŸèƒ½ï¼‰"""
    for i, (role, msg) in enumerate(st.session_state.chat_history):
        div_class = "user-message" if role == "user" else "ai-message"
        if i == len(st.session_state.chat_history) - 1 and role == "ai" and st.session_state.is_responding:
            st.markdown(
                f'<div class="{div_class} message">{msg}<span class="typing-cursor"></span></div>',
                unsafe_allow_html=True
            )
        else:
            st.markdown(
                f'<div class="{div_class} message">{msg}</div>',
                unsafe_allow_html=True
            )

        # åœ¨æœ€åä¸€æ¡AIæ¶ˆæ¯åæ·»åŠ å¯ç‚¹å‡»çš„æ¨èé—®é¢˜
        if role == "ai" and i == len(st.session_state.chat_history) - 1:
            questions = generate_related_questions(msg)
            with st.container():
                st.markdown("""
                <div style="margin:15px 0 0 10px; padding-left:10px; border-left:2px solid #f0f7ff;">
                    <div style="color:#666; font-size:14px; margin-bottom:8px;">
                        ğŸ” ç›¸å…³é—®é¢˜æ¨è
                    </div>
                """, unsafe_allow_html=True)

                # ä¸ºæ¯ä¸ªé—®é¢˜åˆ›å»ºå¯ç‚¹å‡»æŒ‰é’®
                for q in questions:
                    if st.button(
                            q,
                            key=f"related_q_{hash(q)}",  # ä½¿ç”¨å“ˆå¸Œå€¼ä½œä¸ºå”¯ä¸€é”®
                            help="ç‚¹å‡»è‡ªåŠ¨æé—®",
                            use_container_width=True
                    ):
                        handle_chat_submission(q)
                        st.rerun()

                st.markdown("</div>", unsafe_allow_html=True)


def stream_chat_response(prompt: str, placeholder):
    """å¤„ç†æµå¼APIå“åº”å¹¶å®æ—¶æ›´æ–°ç•Œé¢"""
    api_key = os.getenv("DEEPSEEK_API_KEY")
    if not api_key:
        placeholder.markdown('<div class="ai-message message">âš ï¸ ç³»ç»Ÿé…ç½®é”™è¯¯ï¼šç¼ºå°‘APIå¯†é’¥</div>',
                             unsafe_allow_html=True)
        return

    url = "https://api.deepseek.com/v1/chat/completions"
    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {api_key}'
    }

    data = {
        "model": "deepseek-chat",
        "messages": [{
            "role": "system",
            "content": """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç³–å°¿ç—…å¥åº·ç®¡ç†åŠ©æ‰‹ï¼Œå›ç­”æ—¶è¯·ï¼š
1. ä½¿ç”¨ç®€ä½“ä¸­æ–‡
2. å…ˆç»™å‡ºæ ¸å¿ƒç»“è®ºï¼ˆ1å¥è¯ï¼‰
3. åˆ†ç‚¹è¯´æ˜å…³é”®è¦ç‚¹ï¼ˆæœ€å¤š3ç‚¹ï¼‰
4. æ¯ç‚¹ä¸è¶…è¿‡15å­—
5. æœ€åç”¨ğŸ‘‰æä¾›è¡ŒåŠ¨å»ºè®®
6. ä½¿ç”¨ç®€å•æ˜“æ‡‚çš„è¡¨è¾¾æ–¹å¼"""
        }, {
            "role": "user",
            "content": prompt
        }],
        "stream": True,
        "temperature": 0.5
    }

    full_response = ""
    try:
        with requests.post(url, headers=headers, json=data, stream=True, timeout=10) as response:
            response.raise_for_status()

            for chunk in response.iter_lines():
                if chunk:
                    decoded = chunk.decode('utf-8')
                    if decoded.startswith("data: "):
                        json_str = decoded[6:].strip()
                        if json_str == '[DONE]':
                            break
                        try:
                            chunk_data = json.loads(json_str)
                            if 'content' in chunk_data['choices'][0]['delta']:
                                delta = chunk_data['choices'][0]['delta']['content']
                                full_response += delta
                                placeholder.markdown(
                                    f'<div class="ai-message message">{full_response}<span class="typing-cursor"></span></div>',
                                    unsafe_allow_html=True
                                )
                        except json.JSONDecodeError:
                            continue

        placeholder.markdown(
            f'<div class="ai-message message">{full_response}</div>',
            unsafe_allow_html=True
        )
        return full_response

    except requests.exceptions.RequestException as e:
        error_msg = f"âš ï¸ ç½‘ç»œé”™è¯¯ï¼š{str(e)}" if "HTTPSConnectionPool" in str(e) else f"âš ï¸ è¯·æ±‚å¤±è´¥ï¼š{str(e)}"
        placeholder.markdown(f'<div class="ai-message message">{error_msg}</div>', unsafe_allow_html=True)
        return error_msg
    except Exception as e:
        error_msg = f"âš ï¸ å¤„ç†é”™è¯¯ï¼š{str(e)}"
        placeholder.markdown(f'<div class="ai-message message">{error_msg}</div>', unsafe_allow_html=True)
        return error_msg


def handle_chat_submission(user_input: str):
    """å¤„ç†ç”¨æˆ·æäº¤çš„èŠå¤©æ¶ˆæ¯"""
    if not user_input.strip():
        return

    st.session_state.chat_history.append(("user", user_input.strip()))
    st.session_state.chat_history.append(("ai", "æ€è€ƒä¸­..."))
    st.session_state.is_responding = True

    message_placeholder = st.empty()
    message_placeholder.markdown(
        '<div class="ai-message message">æ€è€ƒä¸­...<span class="typing-cursor"></span></div>',
        unsafe_allow_html=True
    )

    response = stream_chat_response(user_input.strip(), message_placeholder)
    st.session_state.chat_history[-1] = ("ai", response if response else "âš ï¸ æœªèƒ½è·å–å“åº”")
    st.session_state.is_responding = False
    st.rerun()


# -------------------------- ä¸»ç•Œé¢ç»„ä»¶ --------------------------
def render_sidebar():
    """æ¸²æŸ“ä¾§è¾¹æ å†…å®¹"""
    with st.sidebar:
        st.header("ğŸ” é¡¹ç›®çœ‹æ¿")
        st.markdown("""
        **æ•°æ®é›†**: ğŸ“Š Kaggleç³–å°¿ç—…æ•°æ®é›†  
        **æ ¸å¿ƒç®—æ³•**: ğŸ¤– éšæœºæ£®æ—åˆ†ç±»å™¨  
        **ç‰ˆæœ¬ä¿¡æ¯**: ğŸš€ v3.4 | å¼€å‘è€…: é€æœˆé˜Ÿ  
        """)
        st.markdown("---")

        st.subheader("ğŸ“ˆ æ•°æ®æ´å¯Ÿåˆ†æ")
        diabetes_data = load_dataset()

        if not diabetes_data.empty:
            col1, col2 = st.columns(2)
            with col1:
                dist_feature = st.selectbox("åˆ†æç‰¹å¾", options=["è¡€ç³–å€¼", "BMI", "å¹´é¾„", "èƒ°å²›ç´ "])
            with col2:
                chart_type = st.selectbox("å›¾è¡¨ç±»å‹", options=["åˆ†å¸ƒå›¾", "ç®±çº¿å›¾"])

            fig, ax = plt.subplots(figsize=(8, 4))
            try:
                if dist_feature == "è¡€ç³–å€¼":
                    if chart_type == "åˆ†å¸ƒå›¾":
                        sns.histplot(data=diabetes_data, x="Glucose", hue="é£é™©ç­‰çº§", kde=True, palette="viridis",
                                     ax=ax)
                    else:
                        sns.boxplot(data=diabetes_data, x="é£é™©ç­‰çº§", y="Glucose", palette="coolwarm", ax=ax)
                    ax.set_xlabel("è¡€ç³–å€¼ (mg/dL)")
                elif dist_feature == "BMI":
                    sns.violinplot(data=diabetes_data, x="é£é™©ç­‰çº§", y="BMI", palette="Spectral", ax=ax)
                    ax.set_ylabel("BMI æŒ‡æ•°")
                elif dist_feature == "å¹´é¾„":
                    sns.countplot(data=diabetes_data, x="Age", hue="é£é™©ç­‰çº§", palette="rocket", ax=ax)
                    ax.set_xlabel("å¹´é¾„åˆ†å¸ƒ")
                elif dist_feature == "èƒ°å²›ç´ ":
                    sns.scatterplot(data=diabetes_data, x="Insulin", y="Glucose", hue="é£é™©ç­‰çº§", palette="mako", ax=ax)
                    ax.set_xlabel("èƒ°å²›ç´  (Î¼U/mL)")

                plt.title(f"{dist_feature} - é£é™©åˆ†å¸ƒåˆ†æ", fontsize=12)
                ax.grid(alpha=0.3)
                plt.tight_layout()
                st.pyplot(fig)
            except Exception as e:
                st.error(f"å›¾è¡¨ç”Ÿæˆå¤±è´¥: {str(e)}")

            with st.expander("ğŸ“Š æ•°æ®é›†ç»Ÿè®¡æ‘˜è¦"):
                st.dataframe(diabetes_data.describe().T.style.background_gradient(cmap="Blues"))


def render_prediction_form():
    """æ¸²æŸ“é¢„æµ‹è¡¨å•"""
    st.title("ğŸ©¸ ç³–å°¿ç—…é£é™©æ™ºèƒ½é¢„æµ‹ç³»ç»Ÿ")
    st.caption("åŸºäºæœºå™¨å­¦ä¹ çš„ä¸´åºŠè¾…åŠ©å†³ç­–å·¥å…·")

    with st.form("prediction_form"):
        col1, col2, col3 = st.columns(3)

        with col1:
            glucose = st.number_input("è¡€ç³–å€¼ï¼ˆmg/dLï¼‰", 0, 300, 100)
            blood_pressure = st.number_input("è¡€å‹ï¼ˆmmHgï¼‰", 40, 200, 70)
        with col2:
            insulin = st.number_input("èƒ°å²›ç´ ï¼ˆÎ¼U/mLï¼‰", 0, 1000, 80)
            bmi = st.number_input("BMIæŒ‡æ•°", 10.0, 50.0, 25.0, step=0.1)
        with col3:
            dpf = st.number_input("é—ä¼ é£é™©ç³»æ•°", 0.0, 2.5, 0.5, step=0.001)
            age = st.number_input("æ‚£è€…å¹´é¾„", 0, 100, 30)

        if st.form_submit_button("âœ¨ å¼€å§‹æ™ºèƒ½åˆ†æ"):
            with st.spinner('ğŸ” æ­£åœ¨åˆ†ææ•°æ®...'):
                model, scaler = load_models()
                if model is None or scaler is None:
                    st.error("æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œæ— æ³•è¿›è¡Œåˆ†æ")
                    return

                input_data = pd.DataFrame([[glucose, blood_pressure, insulin, bmi, dpf, age]],
                                          columns=["Glucose", "BloodPressure", "Insulin", "BMI",
                                                   "DiabetesPedigreeFunction", "Age"])

                try:
                    scaled_data = scaler.transform(input_data)
                    prediction = model.predict(scaled_data)[0]
                    proba = model.predict_proba(scaled_data)[0][1] * 100

                    st.session_state.prediction_data = {
                        'proba': proba,
                        'risk_level': "é«˜é£é™©" if prediction == 1 else "ä½é£é™©",
                        'glucose': glucose,
                        'blood_pressure': blood_pressure,
                        'bmi': bmi,
                        'insulin': insulin,
                        'age': age
                    }

                    if 'auto_reply_sent' not in st.session_state:
                        st.session_state.auto_reply_sent = False

                    if not st.session_state.auto_reply_sent:
                        st.session_state.auto_reply_sent = True
                        handle_chat_submission(f"æˆ‘çš„ç³–å°¿ç—…é£é™©æ˜¯{proba:.1f}%ï¼Œè¯·ç»™æˆ‘å¥åº·å»ºè®®")

                    risk_color = "#FF6B6B" if prediction == 1 else "#20B2AA"
                    st.markdown(f"""
                    <div class="result-card">
                        <h3 style="color:#2F4F4F; margin-bottom:1rem">ğŸ“Š åˆ†æç»“æœ</h3>
                        <div style="font-size:1.2rem; margin-bottom:1rem">
                            æ‚£ç—…æ¦‚ç‡: <b style="color:{risk_color}">{proba:.1f}%</b>
                        </div>
                        <div style="display: flex; align-items: center; gap: 1rem">
                            {"âš ï¸" if prediction == 1 else "âœ…"}
                            <span style="font-size:1.3rem; color:{risk_color}">
                                {"é«˜é£é™©ï¼å»ºè®®ç«‹å³å°±åŒ»æ£€æŸ¥" if prediction == 1 else "ä½é£é™©ï¼Œè¯·ä¿æŒå¥åº·ä¹ æƒ¯"}
                            </span>
                        </div>
                    </div>
                    """, unsafe_allow_html=True)

                    st.success("åˆ†æå®Œæˆï¼å·²è‡ªåŠ¨ç”Ÿæˆå¥åº·å»ºè®®")
                except Exception as e:
                    logging.error(f"é¢„æµ‹å¤±è´¥: {str(e)}")
                    st.error(f"åˆ†æå¤±è´¥: {str(e)}")


def render_quick_tools():
    """æ¸²æŸ“å¿«æ·å·¥å…·é¢æ¿"""
    st.markdown("### ğŸš€ å¿«æ·å·¥å…·")

    if st.session_state.get('prediction_data'):
        if st.button("è·å–ä¸ªæ€§åŒ–å»ºè®®",
                     key="personal_advice",
                     disabled=st.session_state.is_responding):
            handle_chat_submission(
                f"æ ¹æ®æˆ‘çš„æ£€æµ‹ç»“æœï¼ˆé£é™©{st.session_state.prediction_data['proba']:.1f}%ï¼‰ï¼Œè¯·ç»™æˆ‘ä¸ªæ€§åŒ–å»ºè®®"
            )

    preset_questions = {
        "ç³–å°¿ç—…æ—©æœŸç—‡çŠ¶": "ç³–å°¿ç—…æ—©æœŸæœ‰å“ªäº›å…¸å‹ç—‡çŠ¶ï¼Ÿ",
        "é¥®é£Ÿå»ºè®®": "ç³–å°¿ç—…æ‚£è€…é€‚åˆåƒä»€ä¹ˆé£Ÿç‰©ï¼Ÿ",
        "è¿åŠ¨æ–¹æ¡ˆ": "ç³–å°¿ç—…æ‚£è€…æ¨èçš„è¿åŠ¨æ–¹å¼å’Œé¢‘ç‡ï¼Ÿ",
        "æ£€æµ‹å»ºè®®": "ç³–å°¿ç—…æ‚£è€…åº”è¯¥å®šæœŸåšå“ªäº›æ£€æŸ¥ï¼Ÿ"
    }

    for btn_text, question in preset_questions.items():
        if st.button(btn_text,
                     key=f"preset_{btn_text}",
                     disabled=st.session_state.is_responding):
            handle_chat_submission(question)

    st.markdown("---")
    st.markdown("### ğŸ“Š å¥åº·æ ‡å‡†")
    st.markdown("""
    <div class="health-card">
    <b>è¡€ç³–æŒ‡æ ‡</b>
    - ç©ºè…¹: 4.4-6.1 mmol/L
    - é¤å: <7.8 mmol/L<br>
    <b>BMIæŒ‡æ•°</b>
    - æ­£å¸¸èŒƒå›´: 18.5-24.9
    </div>
    """, unsafe_allow_html=True)


def diabetes_chatbot():
    """ç³–å°¿ç—…èŠå¤©æœºå™¨äººä¸»ç»„ä»¶"""
    with st.container():
        cols = st.columns([0.75, 0.25])

        with cols[0]:
            st.markdown("## ğŸ“ å¥åº·å’¨è¯¢å¯¹è¯")
            render_chat_messages()

            with st.form("chat_form"):
                user_input = st.text_input(
                    "è¾“å…¥æ‚¨çš„é—®é¢˜...",
                    key="user_input",
                    label_visibility="collapsed",
                    placeholder="ä¾‹å¦‚ï¼šç³–å°¿ç—…æ—©æœŸæœ‰å“ªäº›ç—‡çŠ¶ï¼Ÿ"
                )

                submit_button = st.form_submit_button(
                    "å‘é€",
                    disabled=st.session_state.is_responding
                )

                if submit_button and user_input.strip():
                    handle_chat_submission(user_input.strip())

        with cols[1]:
            render_quick_tools()


# -------------------------- ä¸»å‡½æ•° --------------------------
def main():
    """ä¸»å…¥å£å‡½æ•°"""
    initialize_session_state()
    render_sidebar()
    render_prediction_form()
    diabetes_chatbot()


if __name__ == "__main__":
    main()